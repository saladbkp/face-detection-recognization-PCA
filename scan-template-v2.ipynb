{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Scanner\n",
    "\n",
    "This notebook performs face recognition using template matching and PCA features with cosine_similarity checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceScanner:\n",
    "    def __init__(self, model_path, detection_json_path):\n",
    "        \"\"\"\n",
    "        Initialize face scanner\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to trained PCA model\n",
    "            detection_json_path: Path to JSON file generated by detection-v2.py\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.detection_json_path = detection_json_path\n",
    "        self.model_data = None\n",
    "        self.detection_data = None\n",
    "        self.template_image = None\n",
    "        self.is_loaded = False\n",
    "        \n",
    "    def load_model_and_data(self):\n",
    "        \"\"\"\n",
    "        Load PCA model and detection data\n",
    "        \"\"\"\n",
    "        # Load PCA model\n",
    "        if not os.path.exists(self.model_path):\n",
    "            print(f\"Error: Model file {self.model_path} not found!\")\n",
    "            return False\n",
    "        \n",
    "        with open(self.model_path, 'rb') as f:\n",
    "            self.model_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"PCA model loaded: {len(self.model_data['face_features'])} faces, {len(self.model_data['person_id_map'])} persons\")\n",
    "        \n",
    "        # Load detection data\n",
    "        if not os.path.exists(self.detection_json_path):\n",
    "            print(f\"Error: Detection JSON {self.detection_json_path} not found!\")\n",
    "            return False\n",
    "        \n",
    "        with open(self.detection_json_path, 'r', encoding='utf-8') as f:\n",
    "            self.detection_data = json.load(f)\n",
    "        \n",
    "        print(f\"Detection data loaded: {len(self.detection_data['faces'])} detected faces\")\n",
    "        \n",
    "        # Load first face image as template\n",
    "        if self.detection_data['faces']:\n",
    "            first_face = self.detection_data['faces'][0]\n",
    "            template_path = first_face['image_path']\n",
    "            if os.path.exists(template_path):\n",
    "                self.template_image = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)\n",
    "                print(f\"Template image loaded: {template_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Template image not found: {template_path}\")\n",
    "        \n",
    "        self.is_loaded = True\n",
    "        return True\n",
    "    \n",
    "    def get_reference_positions(self, frame_number, tolerance=5):\n",
    "        \"\"\"\n",
    "        Get reference positions around specified frame\n",
    "        \n",
    "        Args:\n",
    "            frame_number: Current frame number\n",
    "            tolerance: Frame number tolerance range\n",
    "        \n",
    "        Returns:\n",
    "            list: List of reference positions\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            return []\n",
    "        \n",
    "        reference_positions = []\n",
    "        \n",
    "        for face_data in self.detection_data['faces']:\n",
    "            face_frame = face_data['frame_number']\n",
    "            \n",
    "            # Check if frame number is within tolerance range\n",
    "            if abs(face_frame - frame_number) <= tolerance:\n",
    "                reference_positions.append({\n",
    "                    'x': face_data['x'],\n",
    "                    'y': face_data['y'],\n",
    "                    'width': face_data['width'],\n",
    "                    'height': face_data['height'],\n",
    "                    'center_x': face_data['center_x'],\n",
    "                    'center_y': face_data['center_y'],\n",
    "                    'frame_diff': abs(face_frame - frame_number)\n",
    "                })\n",
    "        \n",
    "        # Sort by frame difference, prioritize closest frames\n",
    "        reference_positions.sort(key=lambda x: x['frame_diff'])\n",
    "        \n",
    "        return reference_positions\n",
    "    \n",
    "    def template_match_around_position(self, frame, template, ref_pos, search_scale=1.5):\n",
    "        \"\"\"\n",
    "        Perform template matching around reference position\n",
    "        \n",
    "        Args:\n",
    "            frame: Current frame\n",
    "            template: Template image\n",
    "            ref_pos: Reference position\n",
    "            search_scale: Search area expansion factor\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (match position, match score)\n",
    "        \"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        th, tw = template.shape[:2]\n",
    "        \n",
    "        # Calculate search area\n",
    "        search_w = int(ref_pos['width'] * search_scale)\n",
    "        search_h = int(ref_pos['height'] * search_scale)\n",
    "        \n",
    "        # Determine search area boundaries\n",
    "        start_x = max(0, ref_pos['center_x'] - search_w // 2)\n",
    "        end_x = min(w, ref_pos['center_x'] + search_w // 2)\n",
    "        start_y = max(0, ref_pos['center_y'] - search_h // 2)\n",
    "        end_y = min(h, ref_pos['center_y'] + search_h // 2)\n",
    "        \n",
    "        # Extract search region\n",
    "        search_region = frame[start_y:end_y, start_x:end_x]\n",
    "        \n",
    "        if search_region.shape[0] < th or search_region.shape[1] < tw:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_region, template, cv2.TM_CCOEFF_NORMED)\n",
    "        _, max_val, _, max_loc = cv2.minMaxLoc(result)\n",
    "        \n",
    "        # Convert to global coordinates\n",
    "        global_x = start_x + max_loc[0]\n",
    "        global_y = start_y + max_loc[1]\n",
    "        \n",
    "        return (global_x, global_y, tw, th), max_val\n",
    "    \n",
    "    def extract_face_features(self, face_img):\n",
    "        \"\"\"\n",
    "        Extract face features\n",
    "        \n",
    "        Args:\n",
    "            face_img: Face image\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: PCA feature vector\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            return None\n",
    "        \n",
    "        # Preprocessing: convert to grayscale and resize\n",
    "        if len(face_img.shape) == 3:\n",
    "            gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = face_img\n",
    "        \n",
    "        resized = cv2.resize(gray, (64, 64))\n",
    "        flattened = resized.flatten().reshape(1, -1)\n",
    "        \n",
    "        # Normalization and PCA transformation\n",
    "        scaled = self.model_data['scaler'].transform(flattened)\n",
    "        features = self.model_data['pca'].transform(scaled)\n",
    "        \n",
    "        return features[0]\n",
    "    \n",
    "    def recognize_face(self, face_features, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Recognize face\n",
    "        \n",
    "        Args:\n",
    "            face_features: Face feature vector\n",
    "            threshold: Similarity threshold\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (person_id, person_name, confidence)\n",
    "        \"\"\"\n",
    "        if not self.is_loaded or face_features is None:\n",
    "            return -1, \"unknown\", 0.0\n",
    "        \n",
    "        # Calculate similarity with all known faces\n",
    "        similarities = cosine_similarity([face_features], self.model_data['face_features'])[0]\n",
    "        \n",
    "        # Find highest similarity\n",
    "        max_idx = np.argmax(similarities)\n",
    "        max_similarity = similarities[max_idx]\n",
    "        \n",
    "        if max_similarity >= threshold:\n",
    "            person_id = self.model_data['face_labels'][max_idx]\n",
    "            # Find person_name based on person_id\n",
    "            person_name = \"unknown\"\n",
    "            for name, pid in self.model_data['person_id_map'].items():\n",
    "                if pid == person_id:\n",
    "                    person_name = name\n",
    "                    break\n",
    "            return person_id, person_name, max_similarity\n",
    "        else:\n",
    "            return -1, \"unknown\", max_similarity\n",
    "\n",
    "    def process_live_camera(self, show_preview=True):\n",
    "        \"\"\"\n",
    "        Process live camera for real-time face recognition\n",
    "        \n",
    "        Args:\n",
    "            show_preview: Whether to show preview\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            print(\"Error: Model and data not loaded!\")\n",
    "            return\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open camera\")\n",
    "            return\n",
    "        \n",
    "        print(\"Starting live face recognition...\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "        \n",
    "        frame_count = 0\n",
    "        recognition_results = []\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Failed to read frame from camera\")\n",
    "                break\n",
    "            \n",
    "            # Get reference positions (use frame 0 as reference for live mode)\n",
    "            ref_positions = self.get_reference_positions(0, tolerance=10)\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Skip this frame if no template image\n",
    "            if self.template_image is None:\n",
    "                frame_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Store all match results for selecting best match\n",
    "            all_matches = []\n",
    "            \n",
    "            # Process each reference position\n",
    "            for ref_pos in ref_positions:\n",
    "                # Define search area around reference position\n",
    "                search_scale = 2.0  # Larger search area for live mode\n",
    "                search_w = int(ref_pos['width'] * search_scale)\n",
    "                search_h = int(ref_pos['height'] * search_scale)\n",
    "                \n",
    "                # Calculate search area boundaries\n",
    "                height, width = frame.shape[:2]\n",
    "                search_x = max(0, ref_pos['center_x'] - search_w // 2)\n",
    "                search_y = max(0, ref_pos['center_y'] - search_h // 2)\n",
    "                search_x_end = min(width, search_x + search_w)\n",
    "                search_y_end = min(height, search_y + search_h)\n",
    "                \n",
    "                # Adjust search area size\n",
    "                actual_search_w = search_x_end - search_x\n",
    "                actual_search_h = search_y_end - search_y\n",
    "                \n",
    "                if actual_search_w > 0 and actual_search_h > 0:\n",
    "                    # Extract search region\n",
    "                    search_region = gray[search_y:search_y_end, search_x:search_x_end]\n",
    "                    \n",
    "                    # Resize template to match reference position size\n",
    "                    template_resized = cv2.resize(self.template_image, (ref_pos['width'], ref_pos['height']))\n",
    "                    \n",
    "                    # Check if search region is large enough\n",
    "                    if search_region.shape[0] >= template_resized.shape[0] and search_region.shape[1] >= template_resized.shape[1]:\n",
    "                        # Perform template matching\n",
    "                        result = cv2.matchTemplate(search_region, template_resized, cv2.TM_CCOEFF_NORMED)\n",
    "                        _, max_val, _, max_loc = cv2.minMaxLoc(result)\n",
    "                        \n",
    "                        # Convert to global coordinates\n",
    "                        global_x = search_x + max_loc[0]\n",
    "                        global_y = search_y + max_loc[1]\n",
    "                        \n",
    "                        # Store match result\n",
    "                        match_info = {\n",
    "                            'x': global_x,\n",
    "                            'y': global_y,\n",
    "                            'width': ref_pos['width'],\n",
    "                            'height': ref_pos['height'],\n",
    "                            'confidence': max_val,\n",
    "                            'ref_frame_diff': ref_pos['frame_diff']\n",
    "                        }\n",
    "                        all_matches.append(match_info)\n",
    "            \n",
    "            # Select best match (highest confidence)\n",
    "            if all_matches:\n",
    "                best_match = max(all_matches, key=lambda x: x['confidence'])\n",
    "                \n",
    "                # Only process if confidence is above threshold\n",
    "                if best_match['confidence'] > 0.3:  # Lower threshold for live mode\n",
    "                    # Extract face image for recognition\n",
    "                    face_img = frame[best_match['y']:best_match['y']+best_match['height'], \n",
    "                                    best_match['x']:best_match['x']+best_match['width']]\n",
    "                    \n",
    "                    # Extract features and recognize\n",
    "                    features = self.extract_face_features(face_img)\n",
    "                    person_id, person_name, recognition_confidence = self.recognize_face(features)\n",
    "                    \n",
    "                    # Draw result\n",
    "                    color = (0, 255, 0) if person_name != \"unknown\" else (0, 0, 255)\n",
    "                    cv2.rectangle(frame, (best_match['x'], best_match['y']), \n",
    "                                 (best_match['x'] + best_match['width'], best_match['y'] + best_match['height']), color, 2)\n",
    "                    \n",
    "                    # Add label\n",
    "                    label = f\"{person_name} ({recognition_confidence:.2f}) TM:{best_match['confidence']:.2f}\"\n",
    "                    cv2.putText(frame, label, (best_match['x'], best_match['y'] - 10), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            \n",
    "            # Show preview\n",
    "            if show_preview:\n",
    "                cv2.imshow('Live Face Recognition', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        # Clean up resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        print(\"\\nLive recognition stopped!\")\n",
    "        return recognition_results\n",
    "\n",
    "    def process_video(self, video_path, output_path=None, show_preview=True):\n",
    "        \"\"\"\n",
    "        Process video for face recognition\n",
    "        \n",
    "        Args:\n",
    "            video_path: Input video path\n",
    "            output_path: Output video path (optional)\n",
    "            show_preview: Whether to show preview\n",
    "        \"\"\"\n",
    "        if not self.is_loaded:\n",
    "            print(\"Error: Model and data not loaded!\")\n",
    "            return\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {video_path}\")\n",
    "            return\n",
    "        \n",
    "        # Get video information\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        print(f\"Resolution: {width}x{height}, FPS: {fps}, Total frames: {total_frames}\")\n",
    "        \n",
    "        # Setup output video\n",
    "        out = None\n",
    "        if output_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        recognition_results = []\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Get reference positions\n",
    "            ref_positions = self.get_reference_positions(frame_count)\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Skip this frame if no template image\n",
    "            if self.template_image is None:\n",
    "                frame_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Store all match results for selecting best match\n",
    "            all_matches = []\n",
    "            \n",
    "            # Process each reference position\n",
    "            for ref_pos in ref_positions:\n",
    "                # Define search area around reference position\n",
    "                search_scale = 1.5  # Search area expansion factor\n",
    "                search_w = int(ref_pos['width'] * search_scale)\n",
    "                search_h = int(ref_pos['height'] * search_scale)\n",
    "                \n",
    "                # Calculate search area boundaries\n",
    "                search_x = max(0, ref_pos['center_x'] - search_w // 2)\n",
    "                search_y = max(0, ref_pos['center_y'] - search_h // 2)\n",
    "                search_x_end = min(width, search_x + search_w)\n",
    "                search_y_end = min(height, search_y + search_h)\n",
    "                \n",
    "                # Adjust search area size\n",
    "                actual_search_w = search_x_end - search_x\n",
    "                actual_search_h = search_y_end - search_y\n",
    "                \n",
    "                if actual_search_w > 0 and actual_search_h > 0:\n",
    "                    # Extract search region\n",
    "                    search_region = gray[search_y:search_y_end, search_x:search_x_end]\n",
    "                    \n",
    "                    # Resize template to match reference position size\n",
    "                    template_resized = cv2.resize(self.template_image, (ref_pos['width'], ref_pos['height']))\n",
    "                    \n",
    "                    # Check if search region is large enough\n",
    "                    if search_region.shape[0] >= template_resized.shape[0] and search_region.shape[1] >= template_resized.shape[1]:\n",
    "                        # Perform template matching\n",
    "                        result = cv2.matchTemplate(search_region, template_resized, cv2.TM_CCOEFF_NORMED)\n",
    "                        _, max_val, _, max_loc = cv2.minMaxLoc(result)\n",
    "                        \n",
    "                        # Convert to global coordinates\n",
    "                        global_x = search_x + max_loc[0]\n",
    "                        global_y = search_y + max_loc[1]\n",
    "                        \n",
    "                        # Store match result\n",
    "                        match_info = {\n",
    "                            'x': global_x,\n",
    "                            'y': global_y,\n",
    "                            'width': ref_pos['width'],\n",
    "                            'height': ref_pos['height'],\n",
    "                            'confidence': max_val,\n",
    "                            'ref_frame_diff': ref_pos['frame_diff']\n",
    "                        }\n",
    "                        all_matches.append(match_info)\n",
    "            \n",
    "            # Select best match (highest confidence)\n",
    "            if all_matches:\n",
    "                best_match = max(all_matches, key=lambda x: x['confidence'])\n",
    "                \n",
    "                # Extract face image for recognition\n",
    "                face_img = frame[best_match['y']:best_match['y']+best_match['height'], \n",
    "                                best_match['x']:best_match['x']+best_match['width']]\n",
    "                \n",
    "                # Extract features and recognize\n",
    "                features = self.extract_face_features(face_img)\n",
    "                person_id, person_name, recognition_confidence = self.recognize_face(features)\n",
    "                \n",
    "                # Record result\n",
    "                result = {\n",
    "                    'frame_number': int(frame_count),\n",
    "                    'timestamp': float(frame_count / fps if fps > 0 else 0),\n",
    "                    'x': int(best_match['x']),\n",
    "                    'y': int(best_match['y']),\n",
    "                    'width': int(best_match['width']),\n",
    "                    'height': int(best_match['height']),\n",
    "                    'person_id': int(person_id),\n",
    "                    'person_name': str(person_name),\n",
    "                    'confidence': float(recognition_confidence),\n",
    "                    'template_match_confidence': float(best_match['confidence']),\n",
    "                    'ref_frame_diff': int(best_match['ref_frame_diff'])\n",
    "                }\n",
    "                recognition_results.append(result)\n",
    "                \n",
    "                # Draw result\n",
    "                color = (0, 255, 0) if person_name != \"unknown\" else (0, 0, 255)\n",
    "                cv2.rectangle(frame, (best_match['x'], best_match['y']), \n",
    "                                (best_match['x'] + best_match['width'], best_match['y'] + best_match['height']), color, 2)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"{person_name} ({recognition_confidence:.2f}) TM:{best_match['confidence']:.2f}\"\n",
    "                cv2.putText(frame, label, (best_match['x'], best_match['y'] - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            \n",
    "            # Show preview\n",
    "            if show_preview:\n",
    "                cv2.imshow('Face Recognition', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            # Write to output video\n",
    "            if out:\n",
    "                out.write(frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Show progress\n",
    "            if frame_count % 100 == 0:\n",
    "                progress = (frame_count / total_frames) * 100\n",
    "                print(f\"Progress: {progress:.1f}% ({frame_count}/{total_frames} frames)\")\n",
    "        \n",
    "        # Clean up resources\n",
    "        cap.release()\n",
    "        if out:\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Save recognition results\n",
    "        results_path = output_path.replace('recognition_output.mp4', 'recognition_results.json') if output_path else \"recognition_results.json\"\n",
    "        results_data = {\n",
    "            'video_path': video_path,\n",
    "            'total_frames': total_frames,\n",
    "            'fps': fps,\n",
    "            'total_recognitions': len(recognition_results),\n",
    "            'processing_date': datetime.now().isoformat(),\n",
    "            'results': recognition_results\n",
    "        }\n",
    "        \n",
    "        with open(results_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nRecognition completed!\")\n",
    "        print(f\"Total recognitions: {len(recognition_results)}\")\n",
    "        print(f\"Results saved to: {results_path}\")\n",
    "        if output_path:\n",
    "            print(f\"Output video saved to: {output_path}\")\n",
    "        \n",
    "        return recognition_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input Section\n",
    "\n",
    "Please provide the required information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose input mode:\n",
      "1. Video file\n",
      "2. Live camera\n",
      "Person name: Joseph_Lai\n",
      "Mode: Video file\n",
      "Video input: Joseph_Lai.mp4\n"
     ]
    }
   ],
   "source": [
    "# Get user input\n",
    "person_name = input(\"Enter person name: \")\n",
    "\n",
    "# Choose input mode\n",
    "print(\"\\nChoose input mode:\")\n",
    "print(\"1. Video file\")\n",
    "print(\"2. Live camera\")\n",
    "mode_choice = input(\"Enter your choice (1 or 2): \")\n",
    "\n",
    "is_live_mode = mode_choice == '2'\n",
    "video_input = None\n",
    "\n",
    "if not is_live_mode:\n",
    "    video_input = input(\"Enter video file path: \")\n",
    "\n",
    "print(f\"Person name: {person_name}\")\n",
    "print(f\"Mode: {'Live camera' if is_live_mode else 'Video file'}\")\n",
    "if video_input:\n",
    "    print(f\"Video input: {video_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: faces/lock_version/Joseph_Lai/face_model.pkl\n",
      "Detection JSON: faces/lock_version/Joseph_Lai/Joseph_Lai_faces_detection.json\n",
      "Output video: faces/lock_version/Joseph_Lai/recognition_output.mp4\n",
      "Video input: videos/Joseph_Lai.mp4\n"
     ]
    }
   ],
   "source": [
    "# Configure paths based on person name\n",
    "base_dir = f\"faces/lock_version/{person_name}\"\n",
    "model_path = f\"{base_dir}/face_model.pkl\"\n",
    "detection_json_path = f\"{base_dir}/{person_name}_faces_detection.json\"\n",
    "output_video = f\"{base_dir}/recognition_output.mp4\"\n",
    "video_input = f\"videos/{video_input}\"\n",
    "\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Detection JSON: {detection_json_path}\")\n",
    "print(f\"Output video: {output_video}\")\n",
    "print(f\"Video input: {video_input}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file found.\n",
      "Detection JSON found.\n",
      "Video file found.\n"
     ]
    }
   ],
   "source": [
    "# Check necessary files\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Error: Model file {model_path} not found!\")\n",
    "    print(\"Please run train-v2.py first to train the model.\")\n",
    "else:\n",
    "    print(\"Model file found.\")\n",
    "\n",
    "if not os.path.exists(detection_json_path):\n",
    "    print(f\"Error: Detection JSON {detection_json_path} not found!\")\n",
    "    print(\"Please run detection-v2.py first to generate detection data.\")\n",
    "else:\n",
    "    print(\"Detection JSON found.\")\n",
    "\n",
    "if not is_live_mode and video_input and not os.path.exists(video_input):\n",
    "    print(f\"Error: Video file {video_input} not found!\")\n",
    "elif not is_live_mode and video_input:\n",
    "    print(\"Video file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA model loaded: 77 faces, 1 persons\n",
      "Detection data loaded: 77 detected faces\n",
      "Template image loaded: faces/lock_version/Joseph_Lai\\face_000000_frame_000000.jpg\n",
      "Model and data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create scanner and load model\n",
    "scanner = FaceScanner(model_path, detection_json_path)\n",
    "\n",
    "# Load model and data\n",
    "if scanner.load_model_and_data():\n",
    "    print(\"Model and data loaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to load model and data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting face recognition...\n",
      "Press 'q' to quit preview window\n",
      "Processing video: videos/Joseph_Lai.mp4\n",
      "Resolution: 480x848, FPS: 30.0, Total frames: 131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 76.3% (100/131 frames)\n",
      "\n",
      "Recognition completed!\n",
      "Total recognitions: 109\n",
      "Results saved to: faces/lock_version/Joseph_Lai/recognition_results.json\n",
      "Output video saved to: faces/lock_version/Joseph_Lai/recognition_output.mp4\n",
      "\n",
      "Recognition summary:\n",
      "  Joseph_Lai: 87 detections\n",
      "  unknown: 22 detections\n"
     ]
    }
   ],
   "source": [
    "# Process video or live camera\n",
    "if scanner.is_loaded:\n",
    "    if is_live_mode:\n",
    "        print(\"\\nStarting live face recognition...\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "        results = scanner.process_live_camera(show_preview=True)\n",
    "    else:\n",
    "        if video_input and os.path.exists(video_input):\n",
    "            print(\"\\nStarting face recognition...\")\n",
    "            print(\"Press 'q' to quit preview window\")\n",
    "            results = scanner.process_video(video_input, output_video, show_preview=True)\n",
    "            \n",
    "            if results:\n",
    "                print(f\"\\nRecognition summary:\")\n",
    "                # Count recognition results\n",
    "                person_counts = {}\n",
    "                for result in results:\n",
    "                    name = result['person_name']\n",
    "                    person_counts[name] = person_counts.get(name, 0) + 1\n",
    "                \n",
    "                for name, count in person_counts.items():\n",
    "                    print(f\"  {name}: {count} detections\")\n",
    "        else:\n",
    "            print(\"Video file not specified or not found!\")\n",
    "else:\n",
    "    print(\"Scanner not loaded properly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1802d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
