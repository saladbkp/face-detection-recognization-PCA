{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e951dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f0f6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained PCA model from file\n",
    "def load_pca_model(model_path):\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"PCA model loaded successfully from: {model_path}\")\n",
    "        print(f\"Person: {model_data['person_name']}\")\n",
    "        print(f\"Training timestamp: {model_data['training_timestamp']}\")\n",
    "        print(f\"Number of components: {model_data['n_components']}\")\n",
    "        print(f\"Face dimensions: {model_data['face_dimensions']}\")\n",
    "        \n",
    "        return model_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PCA model: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d0049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both dark and light PCA models\n",
    "def load_dual_pca_models(dark_model_path, light_model_path):\n",
    "    print(\"=== Loading Dual PCA Models ===\")\n",
    "    \n",
    "    dark_model = load_pca_model(dark_model_path)\n",
    "    light_model = load_pca_model(light_model_path)\n",
    "    \n",
    "    if dark_model is None or light_model is None:\n",
    "        print(\"Error: Failed to load one or both models\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Both models loaded successfully!\")\n",
    "    return dark_model, light_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38106aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face templates from the faces directory\n",
    "\n",
    "def load_face_templates(faces_dir=\"faces\"):\n",
    "    templates = []\n",
    "    template_info = []\n",
    "    \n",
    "    # Get all subdirectories \n",
    "    subdirs = [d for d in os.listdir(faces_dir) if os.path.isdir(os.path.join(faces_dir, d))]\n",
    "    \n",
    "    print(f\"Loading face templates from: {faces_dir}\")\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(faces_dir, subdir)\n",
    "        # Get all .jpg files\n",
    "        jpg_files = glob.glob(os.path.join(subdir_path, \"*.jpg\"))\n",
    "        \n",
    "        print(f\"Found {len(jpg_files)} template images in {subdir}\")\n",
    "        \n",
    "        for jpg_file in jpg_files[:10]:  # Limit to first 10 templates\n",
    "            try:\n",
    "                # Load template image\n",
    "                template_img = cv2.imread(jpg_file, cv2.IMREAD_GRAYSCALE)\n",
    "                if template_img is not None:\n",
    "                    templates.append(template_img)\n",
    "                    template_info.append({\n",
    "                        'path': jpg_file,\n",
    "                        'subdir': subdir,\n",
    "                        'filename': os.path.basename(jpg_file)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading template {jpg_file}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(templates)} face templates\")\n",
    "    return templates, template_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b0ad7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Detect faces using template matching\n",
    "\n",
    "def detect_faces_with_template(gray_frame, templates, template_info, threshold=0.7, nms_threshold=0.3):\n",
    "\n",
    "    detections = []\n",
    "    \n",
    "    # Try multiple scales for template matching\n",
    "    scales = [0.5, 0.7, 1.0, 1.3, 1.6]\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Resize frame for current scale\n",
    "        if scale != 1.0:\n",
    "            scaled_frame = cv2.resize(gray_frame, None, fx=scale, fy=scale)\n",
    "        else:\n",
    "            scaled_frame = gray_frame\n",
    "        \n",
    "        for i, template in enumerate(templates):\n",
    "            # Skip if template is larger than the scaled frame\n",
    "            if template.shape[0] > scaled_frame.shape[0] or template.shape[1] > scaled_frame.shape[1]:\n",
    "                continue\n",
    "            \n",
    "            # Perform template matching\n",
    "            result = cv2.matchTemplate(scaled_frame, template, cv2.TM_CCOEFF_NORMED)\n",
    "            \n",
    "            # Find locations where matching exceeds threshold\n",
    "            locations = np.where(result >= threshold)\n",
    "            \n",
    "            for pt in zip(*locations[::-1]):\n",
    "                # Calculate bounding box\n",
    "                x = int(pt[0] / scale)\n",
    "                y = int(pt[1] / scale)\n",
    "                w = int(template.shape[1] / scale)\n",
    "                h = int(template.shape[0] / scale)\n",
    "                \n",
    "                # Store detection with confidence score\n",
    "                confidence = result[int(pt[1])][int(pt[0])]\n",
    "                detections.append((x, y, w, h, confidence))\n",
    "    \n",
    "    # Apply Non-Maximum Suppression to remove overlapping detections\n",
    "    if len(detections) > 0:\n",
    "        # Convert to format expected by cv2.dnn.NMSBoxes\n",
    "        boxes = [(x, y, w, h) for x, y, w, h, _ in detections]\n",
    "        scores = [conf for _, _, _, _, conf in detections]\n",
    "        \n",
    "        # Apply NMS\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, scores, threshold, nms_threshold)\n",
    "        \n",
    "        # Filter detections based on NMS results\n",
    "        if len(indices) > 0:\n",
    "            indices = indices.flatten()\n",
    "            filtered_detections = [(detections[i][0], detections[i][1], detections[i][2], detections[i][3]) \n",
    "                                 for i in indices]\n",
    "        else:\n",
    "            filtered_detections = []\n",
    "    else:\n",
    "        filtered_detections = []\n",
    "    \n",
    "    return filtered_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d40fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Normalize vectors\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = np.dot(vec1, vec2) / (norm1 * norm2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7396e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project a face vector into the eigenface space\n",
    "def project_face_to_eigenspace(face_vector, eigenfaces, mean_face):\n",
    "    # Center the face by subtracting mean\n",
    "    centered_face = face_vector - mean_face\n",
    "    \n",
    "    # Project onto eigenfaces\n",
    "    projected_face = np.dot(centered_face, eigenfaces)\n",
    "    \n",
    "    return projected_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becd0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize a face using the trained PCA model\n",
    "def recognize_face(face_vector, model_data, similarity_threshold=0.7):\n",
    "\n",
    "    eigenfaces = model_data['eigenfaces']\n",
    "    mean_face = model_data['mean_face']\n",
    "    projected_training_data = model_data['projected_data']\n",
    "    person_name = model_data['person_name']\n",
    "    \n",
    "    # Project the input face to eigenspace\n",
    "    projected_face = project_face_to_eigenspace(face_vector, eigenfaces, mean_face)\n",
    "    \n",
    "    # Calculate similarities with all training faces\n",
    "    similarities = []\n",
    "    for training_face in projected_training_data:\n",
    "        similarity = cosine_similarity(projected_face, training_face)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Get the best match\n",
    "    max_similarity = max(similarities)\n",
    "    \n",
    "    # Check if similarity exceeds threshold\n",
    "    is_recognized = max_similarity >= similarity_threshold\n",
    "    \n",
    "    return person_name, max_similarity, is_recognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ebd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize a face using both dark and light PCA models (OR logic)\n",
    "def recognize_face_dual_model(face_vector, dark_model_data, light_model_data, similarity_threshold=0.7):\n",
    "\n",
    "    # Test with dark model\n",
    "    dark_name, dark_similarity, dark_recognized = recognize_face(\n",
    "        face_vector, dark_model_data, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # Test with light model\n",
    "    light_name, light_similarity, light_recognized = recognize_face(\n",
    "        face_vector, light_model_data, similarity_threshold\n",
    "    )\n",
    "    \n",
    "    # OR logic: recognized if either model recognizes the face\n",
    "    is_recognized = dark_recognized or light_recognized\n",
    "    \n",
    "    # Use the higher similarity as the best confidence\n",
    "    best_confidence = max(dark_similarity, light_similarity)\n",
    "    \n",
    "    # Use the person name from the model with higher similarity\n",
    "    person_name = dark_name if dark_similarity >= light_similarity else light_name\n",
    "    \n",
    "    return person_name, best_confidence, is_recognized, dark_similarity, light_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e456a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Detect and recognize faces in a frame using template matching (single model version)\n",
    "\n",
    "def detect_and_recognize_faces(frame, templates, template_info, model_data, similarity_threshold=0.7, template_threshold=0.7):\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces using template matching\n",
    "    faces = detect_faces_with_template(gray, templates, template_info, template_threshold)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        # Extract face region\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize face to match training data dimensions\n",
    "        face_dim = int(np.sqrt(model_data['face_dimensions']))\n",
    "        face_resized = cv2.resize(face_roi, (face_dim, face_dim))\n",
    "        \n",
    "        # Flatten face to vector\n",
    "        face_vector = face_resized.flatten().astype(np.float64)\n",
    "        \n",
    "        # Recognize face\n",
    "        person_name, confidence, is_recognized = recognize_face(\n",
    "            face_vector, model_data, similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # Print similarity value for debugging\n",
    "        status = \"RECOGNIZED\" if is_recognized else \"UNKNOWN\"\n",
    "        print(f\"Face {i+1} at position ({x}, {y}, {w}x{h}): similarity={confidence:.4f}, threshold={similarity_threshold:.4f}, status={status}\")\n",
    "        \n",
    "        results.append((x, y, w, h, person_name, confidence, is_recognized))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47dd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Detect and recognize faces in a frame using template matching and dual models (dark + light)\n",
    "\n",
    "def detect_and_recognize_faces_dual_model(frame, templates, template_info, dark_model_data, light_model_data, similarity_threshold=0.7, template_threshold=0.7):\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces using template matching\n",
    "    faces = detect_faces_with_template(gray, templates, template_info, template_threshold)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        # Extract face region\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize face to match training data dimensions\n",
    "        face_dim = int(np.sqrt(dark_model_data['face_dimensions']))\n",
    "        face_resized = cv2.resize(face_roi, (face_dim, face_dim))\n",
    "        \n",
    "        # Flatten face to vector\n",
    "        face_vector = face_resized.flatten().astype(np.float64)\n",
    "        \n",
    "        # Recognize face using dual models\n",
    "        person_name, best_confidence, is_recognized, dark_similarity, light_similarity = recognize_face_dual_model(\n",
    "            face_vector, dark_model_data, light_model_data, similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # Print detailed similarity values for debugging\n",
    "        status = \"RECOGNIZED\" if is_recognized else \"UNKNOWN\"\n",
    "        print(f\"Face {i+1} at position ({x}, {y}, {w}x{h}):\")\n",
    "        print(f\"  Dark model:  similarity={dark_similarity:.4f}, threshold={similarity_threshold:.4f}\")\n",
    "        print(f\"  Light model: similarity={light_similarity:.4f}, threshold={similarity_threshold:.4f}\")\n",
    "        print(f\"  Best confidence: {best_confidence:.4f}, Status: {status}\")\n",
    "        \n",
    "        results.append((x, y, w, h, person_name, best_confidence, is_recognized))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87acb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Draw bounding boxes and labels on detected faces\n",
    "\n",
    "def draw_face_annotations(frame, detection_results, avg_face_size=None):\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    for (x, y, w, h, person_name, confidence, is_recognized) in detection_results:\n",
    "        # Skip detection results with confidence < 0.3 and not recognized\n",
    "        if confidence < 0.3 and not is_recognized:\n",
    "            continue\n",
    "            \n",
    "        # Dynamic face size filtering based on average size\n",
    "        if avg_face_size is not None:\n",
    "            current_face_size = (w + h) / 2\n",
    "            if current_face_size < avg_face_size * 0.5:  # Skip faces smaller than 50% of average\n",
    "                continue\n",
    "        else:\n",
    "            # Fallback to original size filtering\n",
    "            if (w + h) / 2 < 200:\n",
    "                continue\n",
    "            \n",
    "        # All face detection boxes are RED SQUARES\n",
    "        box_color = (0, 0, 255)  # Red color for all detection boxes\n",
    "        \n",
    "        # Make it a square by using the larger dimension\n",
    "        size = max(w, h)\n",
    "        # Center the square on the detected face\n",
    "        square_x = x + (w - size) // 2\n",
    "        square_y = y + (h - size) // 2\n",
    "        \n",
    "        # Draw red square bounding box\n",
    "        cv2.rectangle(annotated_frame, (square_x, square_y), (square_x + size, square_y + size), box_color, 2)\n",
    "        \n",
    "        # Choose label color based on recognition status\n",
    "        if is_recognized:\n",
    "            label_color = (255, 255, 0)  # Cyan for recognized faces\n",
    "            label = f\"{person_name} ({confidence:.2f})\"\n",
    "        else:\n",
    "            label_color = (0, 0, 255)  # Red for unknown faces\n",
    "            label = f\"Unknown ({confidence:.2f})\"\n",
    "        \n",
    "        # Draw label background\n",
    "        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "        cv2.rectangle(\n",
    "            annotated_frame,\n",
    "            (x, y - label_size[1] - 10),\n",
    "            (x + label_size[0], y),\n",
    "            label_color,\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        # Draw label text\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            label,\n",
    "            (x, y - 5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (255, 255, 255),\n",
    "            2\n",
    "        )\n",
    "    \n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51c5fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Process video to detect and recognize faces using template matching and dual models\n",
    "\n",
    "def process_video(input_video_path, dark_model_path, light_model_path, output_video_path, similarity_threshold=0.7, template_threshold=0.7):\n",
    "\n",
    "    # Load dual PCA models\n",
    "    dark_model_data, light_model_data = load_dual_pca_models(dark_model_path, light_model_path)\n",
    "    if dark_model_data is None or light_model_data is None:\n",
    "        return False\n",
    "    \n",
    "    # Load face templates\n",
    "    templates, template_info = load_face_templates()\n",
    "    if len(templates) == 0:\n",
    "        print(\"Error: No face templates loaded\")\n",
    "        return False\n",
    "    \n",
    "    # Open input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Processing video: {input_video_path}\")\n",
    "    print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "    print(f\"Using {len(templates)} face templates for detection\")\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # First pass: collect face sizes for dynamic filtering\n",
    "    print(\"First pass: collecting face sizes...\")\n",
    "    face_sizes = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 10 == 0:  # Sample every 10th frame for efficiency\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detect_faces_with_template(gray, templates, template_info, template_threshold)\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                face_sizes.append((w + h) / 2)\n",
    "    \n",
    "    # Calculate average face size\n",
    "    avg_face_size = np.mean(face_sizes) if face_sizes else None\n",
    "    print(f\"Average face size: {avg_face_size:.2f}\" if avg_face_size else \"No faces detected in sampling\")\n",
    "    \n",
    "    # Reset video capture for second pass\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    \n",
    "    # Second pass: process frames with dynamic filtering\n",
    "    print(\"Second pass: processing frames...\")\n",
    "    frame_count = 0\n",
    "    recognition_stats = {'recognized': 0, 'unknown': 0, 'total_detections': 0}\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Detect and recognize faces using template matching and dual models\n",
    "        detection_results = detect_and_recognize_faces_dual_model(\n",
    "            frame, templates, template_info, dark_model_data, light_model_data, \n",
    "            similarity_threshold, template_threshold\n",
    "        )\n",
    "        \n",
    "        # Update statistics\n",
    "        for (_, _, _, _, _, _, is_recognized) in detection_results:\n",
    "            recognition_stats['total_detections'] += 1\n",
    "            if is_recognized:\n",
    "                recognition_stats['recognized'] += 1\n",
    "            else:\n",
    "                recognition_stats['unknown'] += 1\n",
    "        \n",
    "        # Draw annotations with dynamic filtering\n",
    "        annotated_frame = draw_face_annotations(frame, detection_results, avg_face_size)\n",
    "        \n",
    "        out.write(annotated_frame)\n",
    "        \n",
    "        if frame_count % 30 == 0:  # Print every 30 frames\n",
    "            progress = (frame_count / total_frames) * 100\n",
    "            print(f\"Progress: {progress:.1f}% ({frame_count}/{total_frames})\")\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"\\n=== Processing Complete ===\")\n",
    "    print(f\"Output video saved to: {output_video_path}\")\n",
    "    print(f\"Total frames processed: {frame_count}\")\n",
    "    print(f\"Total face detections: {recognition_stats['total_detections']}\")\n",
    "    print(f\"Recognized faces: {recognition_stats['recognized']}\")\n",
    "    print(f\"Unknown faces: {recognition_stats['unknown']}\")\n",
    "    \n",
    "    if recognition_stats['total_detections'] > 0:\n",
    "        recognition_rate = (recognition_stats['recognized'] / recognition_stats['total_detections']) * 100\n",
    "        print(f\"Recognition rate: {recognition_rate:.1f}%\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5094a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Process live camera feed to detect and recognize faces in real-time using template matching and dual models\n",
    "\n",
    "def process_live_camera(dark_model_path, light_model_path, similarity_threshold=0.7, template_threshold=0.7):\n",
    "\n",
    "    # Load dual PCA models\n",
    "    dark_model_data, light_model_data = load_dual_pca_models(dark_model_path, light_model_path)\n",
    "    if dark_model_data is None or light_model_data is None:\n",
    "        return False\n",
    "    \n",
    "    # Load face templates\n",
    "    templates, template_info = load_face_templates()\n",
    "    if len(templates) == 0:\n",
    "        print(\"Error: No face templates loaded\")\n",
    "        return False\n",
    "    \n",
    "    # Open camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Live camera face recognition started. Press 'q' to quit.\")\n",
    "    print(f\"Using {len(templates)} face templates for detection\")\n",
    "    \n",
    "    # Initialize face size history for dynamic filtering\n",
    "    face_size_history = []\n",
    "    max_history_size = 50\n",
    "    \n",
    "    # Process frames in real-time\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame from camera\")\n",
    "            break\n",
    "        \n",
    "        # Detect and recognize faces using template matching and dual models\n",
    "        detection_results = detect_and_recognize_faces_dual_model(\n",
    "            frame, templates, template_info, dark_model_data, light_model_data, \n",
    "            similarity_threshold, template_threshold\n",
    "        )\n",
    "        \n",
    "        # Update face size history for dynamic filtering\n",
    "        for (x, y, w, h, _, _, _) in detection_results:\n",
    "            face_size_history.append((w + h) / 2)\n",
    "        \n",
    "        # Maintain history size limit\n",
    "        if len(face_size_history) > max_history_size:\n",
    "            face_size_history = face_size_history[-max_history_size:]\n",
    "        \n",
    "        # Calculate dynamic average face size\n",
    "        avg_face_size = np.mean(face_size_history) if face_size_history else None\n",
    "        \n",
    "        annotated_frame = draw_face_annotations(frame, detection_results, avg_face_size)\n",
    "        \n",
    "        cv2.imshow('Live Face Recognition (Template Matching) - Press q to quit', annotated_frame)\n",
    "        \n",
    "        # Check for 'q' key press to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Live camera face recognition stopped.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec44dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Dual model paths\n",
    "dark_model_file = \"models/Joseph_Lai_dark_pca_model.pkl\"\n",
    "light_model_file = \"models/Joseph_Lai_light_pca_model.pkl\"\n",
    "similarity_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdce7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both model files exist\n",
    "if not os.path.exists(dark_model_file):\n",
    "    raise FileNotFoundError(f\"Dark PCA model not found: {dark_model_file}\")\n",
    "if not os.path.exists(light_model_file):\n",
    "    raise FileNotFoundError(f\"Light PCA model not found: {light_model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0687d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if u using camera run this\n",
    "# Live camera mode\n",
    "print(\"Starting live camera mode with template matching...\")\n",
    "success = process_live_camera(dark_model_file, light_model_file, similarity_threshold, template_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5c7bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video file with template matching: C:\\Users\\Asus\\Desktop\\face_detection\\videos\\test.mp4\n",
      "=== Loading Dual PCA Models ===\n",
      "PCA model loaded successfully from: models/Joseph_Lai_dark_pca_model.pkl\n",
      "Person: Joseph_Lai\n",
      "Training timestamp: 2025-08-20T04:03:56.181543\n",
      "Number of components: 50\n",
      "Face dimensions: 10000\n",
      "PCA model loaded successfully from: models/Joseph_Lai_light_pca_model.pkl\n",
      "Person: Joseph_Lai\n",
      "Training timestamp: 2025-08-20T04:03:56.502150\n",
      "Number of components: 50\n",
      "Face dimensions: 10000\n",
      "Both models loaded successfully!\n",
      "Loading face templates from: faces\n",
      "Found 512 template images in Dark_version\n",
      "Found 229 template images in Light_version\n",
      "Successfully loaded 20 face templates\n",
      "Processing video: C:\\Users\\Asus\\Desktop\\face_detection\\videos\\test.mp4\n",
      "Video properties: 960x544, 30 FPS, 184 frames\n",
      "Using 20 face templates for detection\n",
      "First pass: collecting face sizes...\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'greater_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.StrDType'>) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Temp/ipykernel_17316/152327977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Processing video file with template matching: {input_video}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m success = process_video(input_video, dark_model_file, light_model_file, output_video, \n\u001b[0m\u001b[0;32m     18\u001b[0m                       similarity_threshold, template_threshold)\n",
      "\u001b[1;32mC:\\Temp/ipykernel_17316/4277947982.py\u001b[0m in \u001b[0;36mprocess_video\u001b[1;34m(input_video_path, dark_model_path, light_model_path, output_video_path, similarity_threshold, template_threshold)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Sample every 10th frame for efficiency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_faces_with_template\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Temp/ipykernel_17316/3265877034.py\u001b[0m in \u001b[0;36mdetect_faces_with_template\u001b[1;34m(gray_frame, templates, template_info, threshold, nms_threshold)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# Find locations where matching exceeds threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mlocations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlocations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'greater_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.Float32DType'>, <class 'numpy.dtypes.StrDType'>) -> None"
     ]
    }
   ],
   "source": [
    "input_video = \"C:\\\\Users\\\\Asus\\\\Desktop\\\\face_detection\\\\videos\\\\test.mp4\"\n",
    "output_dir = \"output\"\n",
    "template_threshold = \"video\"\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate output video filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "video_name = \"test\"\n",
    "output_video = os.path.join(output_dir, f\"recognized_template_{video_name}_{timestamp}.mp4\")\n",
    "\n",
    "# Check if input video exists\n",
    "if not os.path.exists(input_video):\n",
    "    raise FileNotFoundError(f\"Input video not found: {input_video}\")\n",
    "\n",
    "print(f\"Processing video file with template matching: {input_video}\")\n",
    "success = process_video(input_video, dark_model_file, light_model_file, output_video, \n",
    "                      similarity_threshold, template_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    print(f\"\\n=== Face Recognition Complete ===\")\n",
    "    print(f\"Input video: {input_video}\")\n",
    "    print(f\"Output video: {output_video}\")\n",
    "else:\n",
    "    print(\"Face recognition failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
